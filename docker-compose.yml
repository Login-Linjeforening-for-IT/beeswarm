services:
  llama-server:
    build: .
    container_name: llama-server
    ports:
      - "8080:8080"
    volumes:
      - ./llama.cpp:/app/llama.cpp
    restart: always
